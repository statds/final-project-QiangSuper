---
title: "Parallel Computing for MCMC in R"
author: "Tairan Ye"
date: "April 10, 2018"
output:
  ioslides_presentation: default
  beamer_presentation: default
  slidy_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(doParallel)
library(foreach)
library(boot)
```

## Basic Procedure
<img src="C:/Users/Qiangsuper/Dropbox/Data Science/Parallel computing/Procedure of parallel computing.jpg" width="780" />

## Motivation
- Conduct a simulation study with different sets of parameters. 
- Run multiple chains of Markov Chain Monte Carlo sampling algorithm simultaneously. 
- Other sampling algorithm, like bootstrapping, cross-validation, and so on.  

In all these situations, you need to repeat a computation over times in R. Basically, all these jobs can be achieved by `for` loop.  

However, if a huge amount of computations are conducted, or if each individual computation is time-consuming, a `for` loop will be quite slow. 

## Real Case
- In my simulation study, I propose MCMC approach to the Tweedie model, where I set up a simulated data with 400 records. 
- However, it takes `R` around 2 days to complete the estimate. 
- The real data from Travelers is significantly larger than my simulated data set. 
- I am working on introducing parallel computing to my algorithm to save time and improve efficiency.  

## Main Reason
Each time, we need to apply Metropolis-Hasting algorithm to update the parameter:

- Sample $u^*$ from proposal distribution, say $u^* \sim J_u(u|u^{(t-1)})$
- Compute the acceptance ratio: $$r=\frac{L(u^*)}{L(u_s)} \times \frac{J_u(u^{(t-1)}|u^*)}{J_u(u^*|u^{(t-1)})}$$
- Sample $v \sim \mathbf{U}(0,1)$ and update $u^t=u^*$ if $v<r$

The computations of likelihood involve `for` loop, which significantly reduce the efficiency.  

## Parallel Backends
Since most of the computers are designed with multicore processors, and most of computations don't need to communicate, we can spread these computations across multiple cores and executed in parallel, reducing computation time.  

By default, `R` won't take advantage of all the cores available on a computer. In order to execute code in parallel, firstly, you need to define the desired number of cores available to R by registering a 'parallel backend', which effectively creates a cluster to which computations can be sent.


## Parallel Backends
In `R`, we can either utilize packages to conduct parallel computing locally, or use the `SparkR` and `Sparklyr` to communicate with `Spark`. 

- `doMC` (built on multicore, works for UNIX-alike)
- `doSNOW` (built on snow, works for Windows)
- `doParallel` (built on parallel, works for both)

The `parallel` package is essentially a combination of `multicore` and `snow` -- it will  automatically apply an appropriate tool for your system.

## Create a parallel backend locally
```{r, echo=TRUE, warning=FALSE}
# Find out how many cores are available 
detectCores()
# Create cluster with desired number of cores
cl <- makeCluster(4)
# Register cluster
registerDoParallel(cl)
# Find out how many cores are being used
getDoParWorkers()
```

## Parallel Computation Function
- `foreach` -- parallel analogue to a standard `for` loop
- `parLapply` -- parallel analogue for the apply family of functions
- `caret` -- automatically paralellize if a backend is registered for classification and regression training, and cross-validation
- `bugsparallel` -- parallel MCMC chains in `WinBUGS` via `R2WinBUGS`
- `dclone` -- parallel BUGS chains for MLE with MCMC approach
- `pls` -- parallel computation for partial least square and principal component regression 
- `plyr` -- optional parallel data manipulation and apply-like function

## `foreach`
The `%dopar%` operator passes the defined computations, here sqrt(i), to the available cores. Perfect substitution for `for` loop. 
```{r, echo=TRUE, warning=FALSE}
result_foreach <- foreach(i = 1:100000) %dopar% sqrt(i)
proc.time()
```
```{r, echo=TRUE, warning=FALSE}
result_for <- NULL
for (j in 1:100000){
  result_for[j] <- sqrt(j)
}
proc.time()
```

## `Sparklyr`
- R interface to Apache Spark, a fast and general engine for big data processing. 
- This package supports connecting to local and remote Apache Spark clusters, provides a `dplyr` compatible back-end, and provides an interface to Spark's built-in machine learning algorithms.
