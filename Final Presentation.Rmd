---
title: "Acceleration of Metropolis-Hasting Algorithm"
author: "Tairan"
date: "April 26, 2018"
output: slidy_presentation
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, warning=FALSE, include=FALSE}
library(LaplacesDemon)
library(invgamma)
library(MASS)
library(tidyr)
library(dplyr)
library(ggplot2)
library(microbenchmark)
library(Rcpp)
library(RcppArmadillo)
library(mvtnorm)

load(file = "project.RData")
```

## Motivation

Compared to the Gibbs sampling and Slice sampling, the Metropolis-Hasting algorithm might be the most inefficient one. 

- Unable to update the sample at every iteration: the optimal acceptance rate is 20%~30%
- Significant computation efficiency bottleneck 

However, in some situations, we have to employ Metropolis-Hasting:

- The posterior distribution is not in a closed form
- The shape of sample distribution might be multi-modals

## Solution from C++

- C++ is a general-purpose programming language, which also provides facilities for low-level memory manipulation. 

- The ideal procedure to analyze the posterior distribution is to generate the sampling distribution in C++ and then study its properties in R with a mass of available packages. 

- The RCpp package provides R functions as well as C++ classes which offer a seamless integration of R and C++. Many R data types and objects can be mapped back and forth to C++ equivalents which facilitates both writing of new code as well as easier integration of third-party libraries. 

## Key Packages in RCpp

### Rcpp

- Rcpp provides a powerful API on top of R, permitting direct interchange of rich R objects between R and C++

- Rcpp sugar gives syntactic sugar such as vectorised C++ expression

- Rcpp modules provide easy extensibility using declarations and Rcpp attributes greatly facilitates code integration.

### RcppArmadillo

- Rcpp connects R with the powerful Armadillo templated C++ library for linear algebra

- Armadillo aims towards a good balance between speed and ease of use, and its syntax is deliberately similar to Matlab which makes it easy to port existing code

[Armadillo help document](http://arma.sourceforge.net/docs.html)

### RcppEigen 

- RcppEigen gives R access to the high-performance Eigen linear algebra library. 

### RInside

- The RInside package provides C++ classes that make it easier to embed R in C++ code

## Model

### Data:

- $Y$, the binary outcome, indicates the occurrence of accident in a specified time period

- $X_1$, the binary variable, records if the driver is charged of a moving violation 

- $X_2$  and $X_3$ are two dummies variables for driving history, which is categorical with 3 levels. 

$$ X_2 = 0 \text{ & } X_3 = 0 \text{ if age } \in (0, 5]  $$
$$ X_2 = 1 \text{ & } X_3 = 0 \text{ if age } \in (5, 15)  $$
$$ X_2 = 0 \text{ & } X_3 = 1 \text{ if age } \in (15, \infty)  $$

- The sample size $n$ is 50

### Model with a Bayesian logistic regression:
$$logit(Y)=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3$$

### The noninformative prior for $\vec{\beta}$:
$$\beta_0, \beta_1, \beta_2, \beta_3 \sim N(0, 10^2)$$

### The posterior distribution can be easily achieved:
$$\log f(\vec{ \beta }|X, Y) \propto \sum \{ y_i \times \log(p_i) + (1-y_i) \times \log(1-p_i) \} - \frac{ \vec{ \beta }^\intercal \vec{ \beta }}{2 \times 10}$$

- Obviously no conjugacy and even closed form here -- Metropolis-Hasting is employed

## Sampling 

### The sampling procedure:

- Sample the proposal distribution, which is centered and symmetric around $\beta^{(s)}$ 
$$ \beta^* \sim N(\beta^{(s)}, \sigma_p^2) $$

- Compute the acceptance ratio:

$$r=\frac{ f({ \beta^* }|X, Y)}{ f({ \beta^{(s)} }|X, Y)} *
				\frac{ N(\beta^{(s)}|\beta^*) }{ N(\beta^*|\beta^{(s)})} $$

- Update the sequence
$$\beta^{(s+1)}=\beta^* \text{ if } r>u \sim U(0,1) $$ 
$$\beta^{(s+1)}=\beta^{(s)} \text{ if } r<u \sim U(0,1) $$ 

### Algorithm:

- set a starting point to $\beta$

- build a *for* loop to sequentially update $\beta$ with above sampling procedure

- manipulate the MCMC chain -- burn-in, gap, and relative test

### Three approach:

- Complete all the sampling in R


```{r sample1, echo=TRUE, eval=FALSE}
# compute the log posterior of beta vector
log_posterior<-function(beta, X, Y){

  # calculate likelihood
  p_i <- invlogit(X %*% beta)
  likelihood <- sum(dbern(Y, p_i, log = T))
  
  # calculate prior 
  prior <- dmvn(x = beta, mu = rep(0,p), Sigma = sqrt((10^2)*diag(p)), log = T)
  
  # calculate the posterior
  log_post <- likelihood + prior
  
  return(log_post)
}
```


```{r sample2, eval=FALSE, echo=TRUE}
# Generate the sample distribution

for(i in 2:iteration){
  beta_s <- BETA[i-1, ]
  
  # sample from proposal distribution
  beta_star <- mvrnorm(n = 1, beta_s, Sigma = sqrt(jump_v*diag(p)))
  
  # calculate acceptance probability
  r_num <- log_posterior(beta_star, X, Y ) + 
            dmvnorm(x = beta_s, mean = beta_star, sigma = sqrt(jump_v*diag(p)))
  r_denom <- log_posterior(beta_s, X, Y ) + 
            dmvnorm(x = beta_star, mean = beta_s, sigma = sqrt(jump_v*diag(p)))
  
  r <- r_num - r_denom
  rmin<-min(r,log(1))
  
  # accept or reject proposal
  if( rmin > log(runif(n = 1, min = 0, max = 1)) ){ 
    BETA[i, ] <- beta_star
  }else{
    BETA[i, ] <- beta_s
  }
  accept[i] <- rmin
}
```



- Compute the likelihood in C++ and conduct the update procedure, *for* loop, in R

```{r engine='Rcpp', eval=FALSE, echo=TRUE}
// [[Rcpp::depends(RcppArmadillo)]]
#include <RcppArmadillo.h>
#include <Rcpp.h>
#include <math.h>
using namespace Rcpp;
using namespace arma;

// [[Rcpp::export]]
double log_post(vec b, vec Y, mat X, vec b_s, double jump_v) {
  int n = X.n_rows;
  int p = X.n_cols;
  vec xb;
  vec pi;
  double likelihood;
  double prior;
  double proposal;
  double log_posterior;
  
  // linear predictor
  xb =  X*b;
  pi = invlogit(xb);
  
  // compute log likelihood 
  likelihood = 0;
  for(int i=0; i<n; i++){
    likelihood = likelihood + R::dbinom(Y[i], 1, pi[i], 1);
  }
  
  // compute log prior 
  prior=0;
  for(int i=0; i<p; i++){
    prior = prior + R::dnorm4(b[i], 0, 10, 1);
  }
  
  // compute the proposal density
  sd = diagmat(sqrt(jump_v) * ones<vec>(p));
  proposal = normpdf(b_s, b, sd);
  
  // evaluate log posterior as sum of likelihood and prior
  log_posterior = likelihood + prior + proposal;
  return log_posterior;
}
```

- Complete all the sampling in C++

```{r engine='Rcpp', eval=FALSE, echo=TRUE}
// [[Rcpp::export]]
List for_log_post(mat X, vec Y, int iter, double jump_v){
    
    int p = X.n_cols;
    mat beta_shell(iter, p);
    vec accept_result(iter);
    
    // starting value
    beta_result.row(0) = ones<vec>(p).t();
    vec beta_0(p);
    mat sd(p, p);
    vec beta_star(p);
    
    double r_denum = 0;
    double r_num = 0;
    double r = 0;
    double rmin = 0;
    double uni = 0;
    
    for(int i = 1; i < iter; i++){
        beta_0 = trans(beta_result.row(i-1));
        sd = diagmat(sqrt(jump_v) * ones<vec>(p));

        // sample from proposal distribution
        beta_star = mvnrnd(beta_0, sd);
        // ratio computation
        r_num = log_post(beta_star, Y, X, beta_0);
        r_denum = log_post(beta_0, Y, X, beta_Star);
        // acceptance ratio
        r = exp(r_num - r_denum);
        rmin = fmin(r, 1);
        
        //decision
        uni = randu<double>();
        if (rmin > uni){
            beta_result.row(i) = beta_star.t();
        } else {
            beta_result.row(i) = beta_0.t();
        }
        accept_result(i) = rmin;
    }
    List result;
    result["beta_result"] = beta_result;
    result["accept_result"] = accept_result;
    return result;
}
```

## Result

### Traceplots:

- Pure R

```{r, warning=FALSE}
par(mfrow=c(2,2))
plot(res_mh[[1]][burnin:iter ,'intercept'], type='l',
     xlab='MH Iteration', ylab='Posterior Draw', main='Intercept')
abline(h= -1, col='red')
plot(res_mh[[1]][burnin:iter ,'age_1'], type='l',
     xlab='MH Iteration', ylab='Posterior Draw', main='age1')
abline(h= .7, col='red')
plot(res_mh[[1]][burnin:iter ,'age_2'], type='l',
     xlab='MH Iteration', ylab='Posterior Draw', main='age2')
abline(h= 1.1, col='red')
plot(res_mh[[1]][burnin:iter ,'trt'], type='l',
     xlab='MH Iteration', ylab='Posterior Draw', main='trt')
abline(h= 1.1, col='red')
par(mfrow=c(1,1))
```

- Only computation of likelihood in C++

```{r, warning=FALSE}
par(mfrow=c(2,2))
plot(res_mh_cpp$beta_shell[, 1], type='l',
     xlab='MH Iteration', ylab='Posterior Draw', main='Intercept')
abline(h= -1, col='red')
plot(res_mh_cpp$beta_shell[, 2], type='l',
     xlab='MH Iteration', ylab='Posterior Draw', main='age1')
abline(h= .7, col='red')
plot(res_mh_cpp$beta_shell[, 3], type='l',
     xlab='MH Iteration', ylab='Posterior Draw', main='age2')
abline(h= 1.1, col='red')
plot(res_mh_cpp$beta_shell[, 4], type='l',
     xlab='MH Iteration', ylab='Posterior Draw', main='trt')
abline(h= 1.1, col='red')
par(mfrow=c(1,1))
```
			
- Pure C++

```{r, warning=FALSE}
par(mfrow=c(2,2))
plot(res_pure_cpp$beta_shell[, 1], type='l',
     xlab='MH Iteration', ylab='Posterior Draw', main='Intercept')
abline(h= -1, col='red')
plot(res_pure_cpp$beta_shell[, 2], type='l',
     xlab='MH Iteration', ylab='Posterior Draw', main='age1')
abline(h= .7, col='red')
plot(res_pure_cpp$beta_shell[, 3], type='l',
     xlab='MH Iteration', ylab='Posterior Draw', main='age2')
abline(h= 1.1, col='red')
plot(res_pure_cpp$beta_shell[, 4], type='l',
     xlab='MH Iteration', ylab='Posterior Draw', main='trt')
abline(h= 1.1, col='red')
par(mfrow=c(1,1))
```

### Running Time:

The R function *microbenchmark* is used here to measure the running time accurately, which is a good substitution for the *system.time(replicate(repeat_time, function))* expression. The result is as following:

```{r, warning=FALSE, error=FALSE, message=FALSE}
benchmark_result
autoplot(benchmark_result, title = "Running time in milliseconds") + scale_x_discrete(labels = c('Pure R', 'RCpp', 'Pure Cpp'))
```

## Running Time against Sample Size
In previous case, we study the case with sample size 50. The table and plot below presents the relative running time with pure R method as baseline, against the sample size of 50 to 500 in increment of 50.

```{r}
rel_time_summary
ggplot(rel_time_summary, aes(rel_time_summary$`Sample Size`))+
  geom_line(aes(y = rel_time_summary$`Rcpp/R`, colour = 'Rcpp/R'))+ 
  geom_line(aes(y = rel_time_summary$`Cpp/R`, colour = 'Cpp/R'))+
  xlab('Data Sample Size')+ 
  ylab('Relative Runtime Time (Rcpp as baseline)')+
  ggtitle('Relative Running Time against Sample Size')
```


